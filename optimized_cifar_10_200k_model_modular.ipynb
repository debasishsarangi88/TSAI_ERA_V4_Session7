{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimized CIFAR-10 Model with Advanced Data Augmentation (Modular Version)\n",
        "\n",
        "**Target**: <200k parameters, RF>44, >85% accuracy ✅ **VERY CLOSE (84.08%)**\n",
        "\n",
        "**Model Specifications**:\n",
        "- **Parameters**: 150,690 (<200k constraint ✅)\n",
        "- **Receptive Field**: 67 (>44 requirement ✅)\n",
        "- **Peak Accuracy**: 84.08% (very close to 85% target)\n",
        "- **Architecture**: Depthwise separable convolutions with optimized channel progression\n",
        "\n",
        "**Advanced Techniques**:\n",
        "- **MixUp**: Linear interpolation between training examples and labels (Zhang et al., 2017)\n",
        "- **RICAP**: Random Image Cropping and Patching (Takahashi et al., 2018)\n",
        "- **CutMix**: Cut and paste augmentation\n",
        "- **Efficient Architecture**: Optimized for parameter efficiency\n",
        "\n",
        "**Architecture**: Efficient CNN with depthwise separable convolutions\n",
        "- **Initial Conv**: 3→20 channels, 32×32→32×32\n",
        "- **Layer 1**: 2× EfficientBlock(20→20, stride=1), 32×32→32×32\n",
        "- **Layer 2**: 2× EfficientBlock(20→40, stride=2), 32×32→16×16  \n",
        "- **Layer 3**: 2× EfficientBlock(40→80, stride=2), 16×16→8×8\n",
        "- **Layer 4**: 2× EfficientBlock(80→160, stride=2), 8×8→4×4\n",
        "- **Classifier**: Global Average Pooling + Linear(160→10)\n",
        "\n",
        "**Key Features**:\n",
        "- Depthwise separable convolutions for maximum parameter efficiency\n",
        "- Advanced data augmentation with MixUp, RICAP, and CutMix\n",
        "- Label smoothing and proper regularization\n",
        "- Optimized learning rate scheduling\n",
        "- No SE attention blocks to maintain parameter constraint\n",
        "- **EMA bug fixed**: Model selection now uses correct test accuracy (not EMA)\n",
        "\n",
        "**Modular Structure**:\n",
        "- `config.py`: All configuration parameters\n",
        "- `models.py`: Neural network architectures\n",
        "- `augmentation.py`: Data augmentation techniques\n",
        "- `training.py`: Training functions and utilities\n",
        "- `utils.py`: Utility functions and helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install albumentations --quiet\n",
        "%pip install torch torchvision torchaudio --quiet\n",
        "%pip install timm --quiet\n",
        "\n",
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import math\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import time\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import copy\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our modular components\n",
        "from config import *\n",
        "from models import OptimizedCIFAR10Net200K, calculate_receptive_field, count_parameters, print_model_summary\n",
        "from augmentation import get_albumentations_transforms, get_augmentation_techniques\n",
        "from training import train_model_advanced, test_epoch, evaluate_with_tta, LabelSmoothingCrossEntropy\n",
        "from utils import set_seed, get_device, load_cifar10_data, plot_training_history, print_training_summary, create_results_dict, save_training_results, print_model_info\n",
        "\n",
        "# Set device and seeds\n",
        "device = get_device()\n",
        "set_seed()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model and verify parameters\n",
        "model = OptimizedCIFAR10Net200K().to(device)\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "rf = calculate_receptive_field(model)\n",
        "\n",
        "print(f'Total parameters: {total_params:,}')\n",
        "print(f'Trainable parameters: {trainable_params:,}')\n",
        "print(f'Parameters < 200k: {\"✅ YES\" if total_params < 200000 else \"❌ NO\"}')\n",
        "print(f'Expected: 150,690 parameters with RF=67')\n",
        "print(f'Receptive Field: {rf}')\n",
        "print(f'RF > 44: {\"✅ YES\" if rf > 44 else \"❌ NO\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data with augmentation\n",
        "train_transform, test_transform = get_albumentations_transforms()\n",
        "train_loader, test_loader = load_cifar10_data(train_transform, test_transform, TRAINING_CONFIG['batch_size'])\n",
        "\n",
        "# Get augmentation techniques\n",
        "augmentation_techniques = get_augmentation_techniques()\n",
        "\n",
        "print(f\"CIFAR-10 class names: {DATASET_CONFIG['class_names']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Execution\n",
        "print_model_info(model, total_params, rf)\n",
        "\n",
        "# Start training\n",
        "start_time = time.time()\n",
        "\n",
        "train_losses, train_accs, test_losses, test_accs, best_acc = train_model_advanced(\n",
        "    model, device, train_loader, test_loader, augmentation_techniques,\n",
        "    epochs=TRAINING_CONFIG['epochs'], lr=TRAINING_CONFIG['learning_rate']\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training completed in {training_time/3600:.2f} hours\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation with TTA\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL EVALUATION WITH TEST TIME AUGMENTATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_model_200k.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Standard evaluation\n",
        "test_loss, test_acc = test_epoch(model, device, test_loader, LabelSmoothingCrossEntropy(smoothing=0.1))\n",
        "print(f\"Standard Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "# TTA evaluation\n",
        "tta_acc = evaluate_with_tta(model, test_loader, device, TTA_CONFIG['num_augmentations'])\n",
        "\n",
        "# Print final results\n",
        "print_training_summary(model, total_params, rf, train_accs, test_accs, best_acc, tta_acc, training_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization and Analysis\n",
        "plot_training_history(train_losses, train_accs, test_losses, test_accs)\n",
        "\n",
        "# Model summary\n",
        "print_model_summary(model)\n",
        "\n",
        "# Save training results\n",
        "results = create_results_dict(\n",
        "    'OptimizedCIFAR10Net200K', total_params, rf, train_accs, test_accs, \n",
        "    best_acc, tta_acc, training_time\n",
        ")\n",
        "\n",
        "save_training_results(results)\n",
        "print(f\"Best model saved to 'best_model_200k.pth'\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
